{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "Il progetto consiste in una pipeline il cui fine ultimo (per ora) è quello di classificare i meme rispetto all'argomento trattato.  \n",
    "Si è scelto di classificare i meme in quattro classi (Sports, Intrattenimento, Politica, Altro).  \n",
    "\n",
    "Task simili sono di vitale importanza per i social media, per esempio recentemente [Instagram ha introdotto una policy](https://about.instagram.com/it-it/blog/announcements/continuing-our-approach-to-political-content-on-instagram-and-threads) secondo cui i post che contengono riferimenti a politica, pubblicati dagli account che l'utente non segue, non verranno visualizzati di default, a meno che l'utente stesso non cambi un flag nelle impostazioni (opt-in).  \n",
    "\n",
    "\n",
    "![instagram_policy](slides/imgs/instagram_policy.webp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "Il dataset è costituito da meme provenienti da reddit. Ho scelto questo social media in quanto ogni post su reddit appartiene ad un sub-reddit che definisce una specifica area di interesse. Per esempio, tutti i meme del subreddit [r/PoliticalMemes](https://www.reddit.com/r/PoliticalMemes/) parlano certamente di politica. In altre parole, ogni post ha una etichetta implicita data dal subreddit di appartenenza.\n",
    "Come anticipato, ho scelto di categorizzare i meme in quattro categorie (Sports, Intrattenimento, Politica, Altro), il seguente codice dichiara un dizionario per mappare i nomi dei rispettivi subreddit ai nomi delle classi che ho scelto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Labels\n",
    "```python\n",
    "categories_id = {\n",
    "    \"sport\": 0,\n",
    "    \"ent\": 1,\n",
    "    \"pol\": 2,\n",
    "    \"oth\": 3\n",
    "}\n",
    "# categories_names = {0: \"sport\", 1: \"ent\", 2: \"pol\", 3: \"oth\"} ovvero categories_id invertito\n",
    "categories_names = {v: k for k, v in categories_id.items()}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Per ogni subreddit la sua label (e viceversa)\n",
    "```python\n",
    "subreddit_categories = {\n",
    "    \"Animemes\": categories_id[\"ent\"],\n",
    "    \"GymMemes\": categories_id[\"sport\"],\n",
    "    \"RelationshipMemes\": categories_id[\"oth\"],\n",
    "    \"PoliticalCompassMemes\": categories_id[\"pol\"],\n",
    "    \"PhilosophyMemes\": categories_id[\"oth\"],\n",
    "    \"CollegeMemes\": categories_id[\"oth\"],\n",
    "    \"HistoryMemes\": categories_id[\"oth\"],\n",
    "    \"TheRightCantMeme\": categories_id[\"pol\"],\n",
    "    \"AnimalMemes\": categories_id[\"oth\"],\n",
    "    \"moviememes\": categories_id[\"ent\"],\n",
    "    \"tvmemes\": categories_id[\"ent\"],\n",
    "    \"musicmemes\": categories_id[\"ent\"],\n",
    "    \"gamingmemes\": categories_id[\"ent\"],\n",
    "    \"PoliticalHumour\": categories_id[\"pol\"],\n",
    "    \"PoliticalMemes\": categories_id[\"pol\"],\n",
    "    \"ScienceHumour\": categories_id[\"oth\"],\n",
    "    \"soccermemes\": categories_id[\"sport\"],\n",
    "    \"footballmemes\": categories_id[\"sport\"],\n",
    "    \"Nbamemes\": categories_id[\"sport\"],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python + Reddit = PRAW <3\n",
    "Verrà utilizzata la libreria PRAW (The Python Reddit API Wrapper) per interfacciarsi con le API di Reddit\n",
    "![praw](./slides/imgs/praw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Panoramica del progetto\n",
    "\n",
    "<img src=\"slides/imgs/project_overview_v2.png\" alt=\"project_structure\" style=\"max-width: 100%; max-height: 90vh; height: auto; display: block; margin: 0 auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First thing first: data ingestion\n",
    "L'interfacciamento con reddit sarà svolto da uno script in python che fa uso della libreria PRAW (Python Reddit API Wrapper) il cui compito sarà quello di fetchare i post in streaming dai subreddits prescelti e inoltrarli a logstash mediante interfaccia HTTP.\n",
    "\n",
    "PRAW offre un metodo per scaricare i post in streaming.\n",
    "\n",
    "```python\n",
    "def get_posts(reddit, subreddits):\n",
    "    for submission in reddit.subreddit(subreddits).stream.submissions():\n",
    "        yield submission\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### data ingestion\n",
    "Ho scelto logstash per la semplicità d'uso e la configurazione semplice e diretta.\n",
    "\n",
    "```conf\n",
    "input {\n",
    "  http {\n",
    "    id => \"tap_http_in\"\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"reddit_posts\"\n",
    "    bootstrap_servers => \"kafkaserver:9092\"\n",
    "  }\n",
    "  # stdout { codec => rubydebug }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![logstash_clown](./slides/imgs/logstash_clown.jpg)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### how much logstash http input plugin is cursed\n",
    "```conf\n",
    "input {\n",
    "  http {\n",
    "    id => \"tap_http_in\"\n",
    "    port => 8081\n",
    "    # https://github.com/logstash-plugins/logstash-input-http/issues/155\n",
    "    additional_codecs => {}\n",
    "    codec => json { target => \"[document]\" }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### filter perchè less is more\n",
    "```conf\n",
    "filter {\n",
    "  # Mantieni solo il campo document\n",
    "  mutate {\n",
    "    remove_field => [\"url\", \"@version\", \"@timestamp\", \"user_agent\", \"event\", \"http\", \"host\"]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### finally output to kafka\n",
    "```conf\n",
    "# https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html#plugins-outputs-kafka-message_key\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"reddit-posts\"\n",
    "    bootstrap_servers => \"kafkaserver:9092\"\n",
    "    message_key => \"%{[document][id]}\"\n",
    "  }\n",
    "  # stdout { codec => rubydebug }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![logstash clown 2](./slides/imgs/logstash_clown_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about images?\n",
    "#### Anatomia di un meme\n",
    "I meme sono un tipo di contenuto multimediale davvero ricco dal punto di vista semantico.  \n",
    "Essi sono costituiti, di solito, da una immagine più del testo. L'analisi delle immagini è quindi centrale quando vogliamo analizzare questo genere di contenuti.\n",
    "![meme_anatomy](./slides/imgs/meme_anatomy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about images?\n",
    "#### Estraiamo del testo: Captioning + OCR\n",
    "Per ottenere dall'immagine del meme la sua didascalia, ho utilizzato un modello di captioning.\n",
    "Un'altra feature importate da estrarre dal meme è il testo contenuto nell'immagine. Perciò utilizzo un OCR.\n",
    "![cap_ocr](./slides/imgs/cap_ocr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Come ho processato le immagini: una breve storia triste.\n",
    "#### Capitolo 1: Image captioning\n",
    "Inizialmente ero affascinato dall'idea di utilizzare i numerosi modelli pretrained presenti su spark nlp. Così per prima cosa ho provato a fare captioning utilizzando [ViT GPT2](https://sparknlp.org/2023/09/20/image_captioning_vit_gpt2_en.html). Il codice per farlo è presente nel file [spark.ipynb](./spark.ipynb).\n",
    "Funziona ma... c'è un enorme \"ma\".\n",
    "```python\n",
    "imageDF = spark.read \\\n",
    "    .format(\"image\") \\\n",
    "    .option(\"dropInvalid\", value = True) \\\n",
    "    .load(IMAGES_PATH)\n",
    "imageAssembler = ImageAssembler() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"image_assembler\",)\n",
    "imageCaptioning = VisionEncoderDecoderForImageCaptioning \\\n",
    "    .pretrained() \\\n",
    "    .setBeamSize(2) \\\n",
    "    .setDoSample(False) \\\n",
    "    .setInputCols([\"image_assembler\"]) \\\n",
    "    .setOutputCol(\"caption\")\n",
    "\n",
    "pipeline = Pipeline().setStages([imageAssembler, imageCaptioning])\n",
    "pipelineDF = pipeline.fit(imageDF).transform(imageDF)\n",
    "pipelineDF \\\n",
    "    .selectExpr(\"reverse(split(image.origin, '/'))[0] as image_name\", \"caption.result\") \\\n",
    "    .show(truncate = False) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non c'è modo di farlo funzionare in streaming\n",
    "#### sarò umile, probabilmente non l'ho trovato io il modo\n",
    "Infatti:\n",
    "- Spark non ha una sorgente per caricare le immagini in streaming, oppure anche se la dovesse avere, non è ufficialmente documentata. L'unica sorgente documentata per le immagini carica tutte le immagini di un determinato folder, tutte in una volta, in un dataframe.\n",
    "```python\n",
    "imageDF = spark.read \\\n",
    "    .format(\"image\") \\\n",
    "    .option(\"dropInvalid\", value = True) \\\n",
    "    .load(IMAGES_PATH)\n",
    "imageAssembler = ImageAssembler() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"image_assembler\",)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non c'è modo di farlo funzionare in streaming\n",
    "#### sarò umile, probabilmente non l'ho trovato io il modo\n",
    "```python\n",
    "imageDF = spark.read \\\n",
    "    .format(\"image\") \\\n",
    "    .option(\"dropInvalid\", value = True) \\\n",
    "    .load(IMAGES_PATH)\n",
    "imageAssembler = ImageAssembler() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"image_assembler\",)\n",
    "```\n",
    "- Anche passando le immagini in binario non sono riuscito a trovare il modo di \"ingannare\" Image Assembler e fargli credere che le immagini che ho appena caricato in binario siano in formato [ImageSchema](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.image.ImageSchema.html) (che è il formato con cui Spark carica le immagini quando usiamo `spark.read.format(\"image\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Come ho processato le immagini: una breve storia triste.\n",
    "#### Capitolo 2: ocr\n",
    "Non è stato possibile neanche fare ocr con spark-nlp, infatti effettuando una ricerca sul sito [sparknlp.org/models](https://sparknlp.org/models) si può constatare che non ci sono modelli che fanno ocr.\n",
    "Forse perchè la johnsnowlabs vende già [spark-ocr](https://nlp.johnsnowlabs.com/docs/en/ocr) che è la soluzione enterprise per fare questo task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Come ho processato le immagini: una breve storia triste.\n",
    "#### Epilogo\n",
    "In conclusione ho scelto di effettuare captioning e ocr all'inizio della pipeline, subito dopo che le immagini vengono scaricate, con uno script python. Per farlo ho utilizzato rispettivamente [blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base)  e [EasyOcr](https://www.jaided.ai/easyocr/tutorial/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![meme_pipeline](./slides/imgs/meme_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data streaming: kafka\n",
    "Ho creato un topic di nome \"reddit-posts\".\n",
    "```yaml\n",
    "    zookeeper:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaZK\n",
    "        environment:\n",
    "            - KAFKA_ACTION=start-zk\n",
    "        networks:\n",
    "            tap:\n",
    "                ipv4_address: 10.0.100.22\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "    kafkaServer:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaServer\n",
    "        environment:\n",
    "            - KAFKA_ACTION=start-kafka\n",
    "        networks:\n",
    "            tap:\n",
    "                ipv4_address: 10.0.100.23\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "        depends_on:\n",
    "            - zookeeper\n",
    "\n",
    "    topic:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaTopic1\n",
    "        environment:\n",
    "            - KAFKA_ACTION=create-topic\n",
    "            - KAFKA_PARTITION=2\n",
    "            - KAFKA_TOPIC=reddit-posts\n",
    "        networks:\n",
    "            tap:\n",
    "        depends_on:\n",
    "            - zookeeper\n",
    "            - kafkaServer\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kafka_meme](./slides/imgs/kafka_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark\n",
    "Spark si occupa della classificazione dei meme, ho suddiviso il codice del training del modello dal codice che processa i dati in streaming, vediamone alcuni pezzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark - modello di classificazione\n",
    "```python\n",
    "df = df.withColumn(\n",
    "    \"all_text\", concat_ws(\" \", df.title, df.selftext, df.ocr_text, df.caption_text)\n",
    ")\n",
    "tokenizer = Tokenizer(inputCol=\"all_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**16)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, tol=1e-6, fitIntercept=True)\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, ovr])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark - streaming\n",
    "Il codice legge lo stream proveniente da kafka e per ogni batch classifica i meme contenuti in esso, poi scrive il risultato su elasticsearch.\n",
    "```python\n",
    "print(\"Reading stream from kafka...\")\n",
    "# Read the stream from kafka\n",
    "kafka_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaServer)\n",
    "    .option(\"subscribe\", topic)\n",
    "    .load()\n",
    ")\n",
    "# omissis\n",
    "df.writeStream.foreachBatch(process).start().awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data processing: Spark - streaming\n",
    "```python\n",
    "def process(batch_df: DataFrame, batch_id: int):\n",
    "    print(\"Processing batch: \", batch_id)\n",
    "    df = batch_df\n",
    "    # omissis\n",
    "    df = df.withColumn(\n",
    "        \"all_text\", concat_ws(\" \", df.title, df.selftext, df.ocr_text, df.caption_text)\n",
    "    )\n",
    "\n",
    "    df1 = classificationModel.transform(df)\n",
    "    # omissis\n",
    "    print(\"Processed batch: \", batch_id, \"writing to elastic\")\n",
    "    df1.write.format(\"es\").mode(\"append\").option(\"es.mapping.id\", \"id\").save(\n",
    "        elastic_index\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data indexing: elasticsearch - mappings are important!\n",
    "Ho creato un ulteriore container che si occupa di creare un indice su elasticsearch e di impostare i mappings per i dati che gli manderò. Quest'ultimo step è stato davvero importante perchè altrimenti su kibana risulta impossibile effettuare alcune visualizzazioni che ho fatto, che richiedevano la tokenizzazione di alcuni campi di testo.\n",
    "```python\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"custom_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\",  # Rimuove le stopwords in inglese\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"created_timestamp\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSSZ||epoch_millis\",\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256,  # Limita la lunghezza massima del campo indicizzato\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": \"standard\",\n",
    "            },\n",
    "            \"selftext\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"caption_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"ocr_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"score\": {\"type\": \"integer\"},\n",
    "            \"upvote_ratio\": {\"type\": \"float\"},\n",
    "            \"subreddit\": {\"type\": \"keyword\"},\n",
    "            \"img_url\": {\"type\": \"keyword\"},\n",
    "            \"img_filename\": {\"type\": \"keyword\"},\n",
    "            \"num_comments\": {\"type\": \"integer\"},\n",
    "            \"predicted_category\": {\"type\": \"keyword\"},\n",
    "            \"ground_truth_category\": {\"type\": \"keyword\"},\n",
    "            \"all_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data visualization: kibana - spoiler alert (next slide)\n",
    "but first let me take a demo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data visualization: kibana - dashboard demo\n",
    "![kibana_dashboard](./slides/imgs/dashboard.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
