{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "Il progetto consiste in una pipeline il cui fine ultimo (per ora) è quello di classificare i meme rispetto all'argomento trattato.  \n",
    "Si è scelto di classificare i meme in quattro classi (Sports, Intrattenimento, Politica, Altro).  \n",
    "\n",
    "Task simili sono di vitale interesse per i social media, per esempio recentemente [Instagram ha introdotto una policy](https://about.instagram.com/it-it/blog/announcements/continuing-our-approach-to-political-content-on-instagram-and-threads) secondo cui i post che contengono riferimenti a politica, pubblicati dagli account che l'utente non segue, non verranno visualizzati di default, a meno che l'utente stesso non cambi un flag nelle impostazioni (opt-in).  \n",
    "\n",
    "\n",
    "![instagram_policy](slides/imgs/instagram_policy.webp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "Il dataset è costituito da meme provenienti da reddit. Ho scelto questo social media in quanto ogni post su reddit appartiene ad un sub-reddit che definisce una specifica area di interesse. Per esempio, tutti i meme del subreddit [r/PoliticalMemes](https://www.reddit.com/r/PoliticalMemes/) parlano certamente di politica. In altre parole, ogni post ha una etichetta implicita data dal subreddit di appartenenza.\n",
    "Come anticipato, ho scelto di categorizzare i meme in quattro categorie (Sports, Intrattenimento, Politica, Altro), il seguente codice dichiara un dizionario per mappare i nomi dei rispettivi subreddit ai nomi delle classi che ho scelto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Labels\n",
    "```python\n",
    "categories_id = {\n",
    "    \"sport\": 0,\n",
    "    \"ent\": 1,\n",
    "    \"pol\": 2,\n",
    "    \"oth\": 3\n",
    "}\n",
    "# categories_names = {0: \"sport\", 1: \"ent\", 2: \"pol\", 3: \"oth\"} ovvero categories_id invertito\n",
    "categories_names = {v: k for k, v in categories_id.items()}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Per ogni subreddit la sua label (e viceversa)\n",
    "```python\n",
    "subreddit_categories = {\n",
    "    \"Animemes\": categories_id[\"ent\"],\n",
    "    \"GymMemes\": categories_id[\"sport\"],\n",
    "    \"RelationshipMemes\": categories_id[\"oth\"],\n",
    "    \"PoliticalCompassMemes\": categories_id[\"pol\"],\n",
    "    \"PhilosophyMemes\": categories_id[\"oth\"],\n",
    "    \"CollegeMemes\": categories_id[\"oth\"],\n",
    "    \"HistoryMemes\": categories_id[\"oth\"],\n",
    "    \"TheRightCantMeme\": categories_id[\"pol\"],\n",
    "    \"AnimalMemes\": categories_id[\"oth\"],\n",
    "    \"moviememes\": categories_id[\"ent\"],\n",
    "    \"tvmemes\": categories_id[\"ent\"],\n",
    "    \"musicmemes\": categories_id[\"ent\"],\n",
    "    \"gamingmemes\": categories_id[\"ent\"],\n",
    "    \"PoliticalHumour\": categories_id[\"pol\"],\n",
    "    \"PoliticalMemes\": categories_id[\"pol\"],\n",
    "    \"ScienceHumour\": categories_id[\"oth\"],\n",
    "    \"soccermemes\": categories_id[\"sport\"],\n",
    "    \"footballmemes\": categories_id[\"sport\"],\n",
    "    \"Nbamemes\": categories_id[\"sport\"],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python + Reddit = PRAW <3\n",
    "Verrà utilizzata la libreria PRAW (The Python Reddit API Wrapper) per interfacciarsi con le API di Reddit\n",
    "![python](./slides/imgs/python.webp) ![praw](./slides/imgs/praw.png) ![reddit](./slides/imgs/reddit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Panoramica del progetto\n",
    "\n",
    "<img src=\"slides/imgs/project_overview_v2.png\" alt=\"project_structure\" style=\"max-width: 100%; max-height: 90vh; height: auto; display: block; margin: 0 auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First thing first: data ingestion\n",
    "L'interfacciamento con reddit sarà svolto da uno script in python che fa uso della libreria PRAW (Python Reddit API Wrapper) il cui compito sarà quello di fetchare i post in streaming dai subreddits prescelti e inoltrarli a logstash mediante interfaccia HTTP.\n",
    "```python\n",
    "def get_posts(reddit, subreddits, t):\n",
    "    for submission in reddit.subreddit(subreddits).stream.submissions():\n",
    "        yield submission\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### data ingestion\n",
    "Ho scelto logstash per la semplicità d'uso e la configurazione semplice e diretta.\n",
    "\n",
    "```conf\n",
    "input {\n",
    "  http {\n",
    "    id => \"tap_http_in\"\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"reddit_posts\"\n",
    "    bootstrap_servers => \"kafkaserver:9092\"\n",
    "  }\n",
    "  # stdout { codec => rubydebug }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![logstash_clown](./slides/imgs/logstash_clown.jpg)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### how much logstash http input plugin is cursed\n",
    "```conf\n",
    "input {\n",
    "  http {\n",
    "    id => \"tap_http_in\"\n",
    "    port => 8081\n",
    "    # https://github.com/logstash-plugins/logstash-input-http/issues/155\n",
    "    additional_codecs => {}\n",
    "    codec => json { target => \"[document]\" }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### filter perchè less is more\n",
    "```conf\n",
    "filter {\n",
    "  # Mantieni solo il campo document\n",
    "  mutate {\n",
    "    remove_field => [\"url\", \"@version\", \"@timestamp\", \"user_agent\", \"event\", \"http\", \"host\"]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### finally output to kafka\n",
    "```conf\n",
    "# https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html#plugins-outputs-kafka-message_key\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"reddit-posts\"\n",
    "    bootstrap_servers => \"kafkaserver:9092\"\n",
    "    message_key => \"%{[document][id]}\"\n",
    "  }\n",
    "  # stdout { codec => rubydebug }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logstash clown 2](./slides/imgs/logstash_clown_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data streaming: kafka\n",
    "```yaml\n",
    "    zookeeper:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaZK\n",
    "        environment:\n",
    "            - KAFKA_ACTION=start-zk\n",
    "        networks:\n",
    "            tap:\n",
    "                ipv4_address: 10.0.100.22\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "    kafkaServer:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaServer\n",
    "        environment:\n",
    "            - KAFKA_ACTION=start-kafka\n",
    "        networks:\n",
    "            tap:\n",
    "                ipv4_address: 10.0.100.23\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "        depends_on:\n",
    "            - zookeeper\n",
    "\n",
    "    topic:\n",
    "        image: tap:kafka\n",
    "        container_name: kafkaTopic1\n",
    "        environment:\n",
    "            - KAFKA_ACTION=create-topic\n",
    "            - KAFKA_PARTITION=2\n",
    "            - KAFKA_TOPIC=reddit-posts\n",
    "        networks:\n",
    "            tap:\n",
    "        depends_on:\n",
    "            - zookeeper\n",
    "            - kafkaServer\n",
    "        profiles: [ \"kafka\", \"ingestion\",\"logstash\", \"pipeline\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kafka_meme](./slides/imgs/kafka_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark\n",
    "Spark si occupa della classificazione dei meme, ho suddiviso il codice del training del modello dal codice che processa i dati in streaming, vediamone alcuni pezzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark - modello di classificazione\n",
    "```python\n",
    "df = df.withColumn(\n",
    "    \"all_text\", concat_ws(\" \", df.title, df.selftext, df.ocr_text, df.caption_text)\n",
    ")\n",
    "tokenizer = Tokenizer(inputCol=\"all_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**16)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, tol=1e-6, fitIntercept=True)\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, ovr])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data processing: Spark - streaming\n",
    "Il codice legge lo stream proveniente da kafka e per ogni batch classifica i meme contenuti in esso, poi scrive il risultato su elasticsearch.\n",
    "```python\n",
    "print(\"Reading stream from kafka...\")\n",
    "# Read the stream from kafka\n",
    "kafka_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaServer)\n",
    "    .option(\"subscribe\", topic)\n",
    "    .load()\n",
    ")\n",
    "# omissis\n",
    "df.writeStream.foreachBatch(process).start().awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data processing: Spark - streaming\n",
    "```python\n",
    "def process(batch_df: DataFrame, batch_id: int):\n",
    "    print(\"Processing batch: \", batch_id)\n",
    "    df = batch_df\n",
    "    # omissis\n",
    "    df = df.withColumn(\n",
    "        \"all_text\", concat_ws(\" \", df.title, df.selftext, df.ocr_text, df.caption_text)\n",
    "    )\n",
    "\n",
    "    df1 = classificationModel.transform(df)\n",
    "    # omissis\n",
    "    print(\"Processed batch: \", batch_id, \"writing to elastic\")\n",
    "    df1.write.format(\"es\").mode(\"append\").option(\"es.mapping.id\", \"id\").save(\n",
    "        elastic_index\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data indexing: elasticsearch - mappings are important!\n",
    "Ho creato un ulteriore container che si occupa di creare un indice su elasticsearch e di impostare i mappings per i dati che gli manderò. Quest'ultimo step è stato davvero importante perchè altrimenti su kibana risulta impossibile effettuare alcune visualizzazioni che ho fatto, che richiedevano la tokenizzazione di alcuni campi di testo.\n",
    "```python\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"custom_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\",  # Rimuove le stopwords in inglese\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"created_timestamp\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSSZ||epoch_millis\",\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256,  # Limita la lunghezza massima del campo indicizzato\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": \"standard\",\n",
    "            },\n",
    "            \"selftext\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"caption_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"ocr_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "            \"score\": {\"type\": \"integer\"},\n",
    "            \"upvote_ratio\": {\"type\": \"float\"},\n",
    "            \"subreddit\": {\"type\": \"keyword\"},\n",
    "            \"img_url\": {\"type\": \"keyword\"},\n",
    "            \"img_filename\": {\"type\": \"keyword\"},\n",
    "            \"num_comments\": {\"type\": \"integer\"},\n",
    "            \"predicted_category\": {\"type\": \"keyword\"},\n",
    "            \"ground_truth_category\": {\"type\": \"keyword\"},\n",
    "            \"all_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                \"analyzer\": \"custom_analyzer\",\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization: kibana\n",
    "but first let me take a demo"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
